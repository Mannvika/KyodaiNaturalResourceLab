{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a67c03-afbd-4eb0-a3fc-338f32adbf41",
   "metadata": {},
   "source": [
    "# 1. Generating InSAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610f358-c488-4391-89e1-f8ad76a65788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac82bd6-47e4-4e3f-9352-42f936019bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InSARSyntheticData(Dataset):\n",
    "    def __init__(self, size, full_dem, baselines, wavelength, S_max=5.0, D=50, nu=0.25, cm=1.0, V=1.0,\n",
    "                 random_noise_std=0.56, tropospheric_noise_beta=1.82, tropospheric_noise_scale=1.0,\n",
    "                 total_days=1460, interval_days=49, f_t=-1, orbit_type='ascending'):\n",
    "        \n",
    "        # Size of image\n",
    "        self.size = size\n",
    "\n",
    "        # Time & Angle parameters\n",
    "        self.total_days = total_days\n",
    "        self.interval_days = interval_days\n",
    "        self.orbit_type = orbit_type\n",
    "        self.incidence_angle_deg, self.satellite_azimuth_deg = self._get_orbit_geometry()\n",
    "        self.los_vector = InSARSyntheticData.calculate_los_vector(self.incidence_angle_deg, self.satellite_azimuth_deg)\n",
    "        self.times = self.get_times()\n",
    "\n",
    "        # Subsidence Parameters\n",
    "        self.S_max = S_max\n",
    "        self.D = D\n",
    "        self.nu = nu\n",
    "        self.cm = cm\n",
    "        self.V = V\n",
    "        self.delta_P_final = -self.S_max * ((np.pi * self.D**2) / (self.cm * (1 - self.nu) * self.V))\n",
    "        self.r, self.subsidence_denominator, self.azimuth = InSARSyntheticData._generate_subsidence_constants(size, D)\n",
    "        self.f_t = f_t\n",
    "\n",
    "        # Guassian noise parameters\n",
    "        self.random_noise_std = random_noise_std\n",
    "\n",
    "        # Tropospheric noise parameters\n",
    "        self.tropospheric_noise_beta = tropospheric_noise_beta\n",
    "        self.tropospheric_noise_scale = tropospheric_noise_scale\n",
    "        self.power_array = InSARSyntheticData._calculate_power_array(size, tropospheric_noise_beta)\n",
    "\n",
    "        # Topographic noise parameters\n",
    "        self.baselines = baselines\n",
    "        self.wavelength = wavelength\n",
    "        self.full_dem = full_dem\n",
    "        SLANT_RANGE = 850000\n",
    "        self.topographic_denominator = self.wavelength * SLANT_RANGE * np.sin(np.deg2rad(self.incidence_angle_deg))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.times)\n",
    "\n",
    "    def generate_subsidence(self, delta_P):\n",
    "        factor = (-1 / np.pi) * self.cm * (1 - self.nu) * delta_P * self.V\n",
    "        uz = factor * (self.D / (self.subsidence_denominator**1.5))\n",
    "        ur = factor * (self.r / (self.subsidence_denominator**1.5))\n",
    "        ux = ur * np.cos(self.azimuth)\n",
    "        uy = ur * np.sin(self.azimuth)\n",
    "        los_vector_calc = self.los_vector\n",
    "        simulated_interferogram = (ux * los_vector_calc[0]) + \\\n",
    "                                  (uy * los_vector_calc[1]) + \\\n",
    "                                  (uz * los_vector_calc[2])\n",
    "        return simulated_interferogram\n",
    "    \n",
    "    def _get_clean_subsidence_image(self, t):\n",
    "        current_time_factor = 0.0\n",
    "        if callable(self.f_t):\n",
    "            if len(self.times) > 0:\n",
    "                current_time_factor = self.f_t(t, self.times, self.total_time)\n",
    "            else:\n",
    "                current_time_factor = t / self.total_time if self.total_time > 0 else 0\n",
    "        else: \n",
    "            current_time_factor = t / self.total_time if self.total_time > 0 else 0\n",
    "        delta_P_current = -self.delta_P_final * current_time_factor\n",
    "        return self.generate_subsidence(delta_P=delta_P_current)\n",
    "\n",
    "    def generate_random_noise(self):\n",
    "        return np.random.normal(loc=0.0, scale=self.random_noise_std, size=self.size)\n",
    "\n",
    "    def generate_tropospheric_noise(self):\n",
    "        noise = np.fft.fft2(np.random.randn(*self.size))\n",
    "        frac_noise = np.fft.ifft2(noise * self.power_array).real\n",
    "        std_val = frac_noise.std()\n",
    "        if std_val > 1e-9:\n",
    "            frac_noise = (frac_noise - frac_noise.mean()) / std_val\n",
    "        else:\n",
    "            frac_noise = frac_noise - frac_noise.mean()\n",
    "        return frac_noise * self.tropospheric_noise_scale\n",
    "\n",
    "    def generate_topographic_noise(self, dem_patch):\n",
    "        delta_h_map = InSARSyntheticData._generate_dem_error(dem_patch, std=5.0)\n",
    "        b_perp = np.random.choice(self.baselines)\n",
    "        numerator = -4 * np.pi * b_perp\n",
    "        if abs(self.topographic_denominator) < 1e-9:\n",
    "            return np.zeros(self.size)\n",
    "        topo_phase_noise = (numerator / self.topographic_denominator) * delta_h_map\n",
    "        return topo_phase_noise\n",
    "\n",
    "    def get_times(self):\n",
    "        if self.total_days < 1 or self.interval_days <=0:\n",
    "             return np.array([1.0])\n",
    "        return np.arange(1, self.total_days + 1, self.interval_days)\n",
    "\n",
    "    def _get_orbit_geometry(self):\n",
    "        if self.orbit_type == 'ascending': return 40, 15\n",
    "        elif self.orbit_type == 'descending': return 40, 195\n",
    "        else: raise ValueError(\"Invalid orbit type.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        max_y = self.full_dem.shape[0] - self.size[0]\n",
    "        max_x = self.full_dem.shape[1] - self.size[1]\n",
    "        \n",
    "        start_y = random.randint(0, max_y)\n",
    "        start_x = random.randint(0, max_x)\n",
    "        \n",
    "        dem_patch = self.full_dem[\n",
    "            start_y : start_y + self.size[0],\n",
    "            start_x : start_x + self.size[1]\n",
    "        ]\n",
    "\n",
    "        current_time = self.times[idx]\n",
    "        \n",
    "        clean_image = self._get_clean_subsidence_image(current_time)\n",
    "        noise1_random = self.generate_random_noise()\n",
    "        noise1_tropo = self.generate_tropospheric_noise()\n",
    "        noise1_topo = self.generate_topographic_noise(dem_patch)\n",
    "        noisy_image1 = clean_image + noise1_random + noise1_tropo + noise1_topo\n",
    "        \n",
    "        noise2_random = self.generate_random_noise()\n",
    "        noise2_tropo = self.generate_tropospheric_noise()\n",
    "        noise2_topo = self.generate_topographic_noise(dem_patch)\n",
    "        \n",
    "        noisy_image2 = clean_image + noise2_random + noise2_tropo + noise2_topo\n",
    "        clean_image_tensor = torch.from_numpy(clean_image.copy()).float().unsqueeze(0)\n",
    "        noisy_image1_tensor = torch.from_numpy(noisy_image1.copy()).float().unsqueeze(0)\n",
    "        noisy_image2_tensor = torch.from_numpy(noisy_image2.copy()).float().unsqueeze(0)\n",
    "        return noisy_image1_tensor, noisy_image2_tensor, clean_image_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_subsidence_constants(size, D):\n",
    "        y, x = np.indices(size)\n",
    "        cx, cy = size[1] // 2, size[0] // 2\n",
    "        r_sq = (x - cx)**2 + (y - cy)**2\n",
    "        r = np.sqrt(r_sq)\n",
    "        denominator_base = r**2 + D**2\n",
    "        denominator_base[denominator_base < 1e-9] = 1e-9\n",
    "        azimuth = np.arctan2(y - cy, x - cx)\n",
    "        return r, denominator_base, azimuth\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_los_vector(incidence_angle_deg, satellite_azimuth_deg):\n",
    "        incidence_angle_rad = np.deg2rad(incidence_angle_deg)\n",
    "        satellite_azimuth_rad = np.deg2rad(satellite_azimuth_deg)\n",
    "        look_azimuth_rad = satellite_azimuth_rad + np.pi/2\n",
    "        l_east = np.sin(incidence_angle_rad) * np.sin(look_azimuth_rad)\n",
    "        l_north = np.sin(incidence_angle_rad) * np.cos(look_azimuth_rad)\n",
    "        l_up = np.cos(incidence_angle_rad)\n",
    "        return np.array([l_east, l_north, l_up])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_power_array(size, beta):\n",
    "        ky = np.fft.fftfreq(size[0])\n",
    "        kx = np.fft.fftfreq(size[1])\n",
    "        kx, ky = np.meshgrid(kx, ky)\n",
    "        k = np.sqrt(kx**2 + ky**2)\n",
    "        k[0, 0] = 1e-7\n",
    "        power = k ** (-beta)\n",
    "        return power\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_dem_error(dem_patch, std=5.0):\n",
    "        smoothed_dem = gaussian_filter(dem_patch, sigma=std)\n",
    "        roughness = dem_patch - smoothed_dem\n",
    "        zero_mean_roughness = roughness - roughness.mean()\n",
    "        roughness_std = zero_mean_roughness.std()\n",
    "        if roughness_std > 1e-9:\n",
    "            normalized_roughness = zero_mean_roughness / roughness_std\n",
    "        else:\n",
    "            normalized_roughness = zero_mean_roughness\n",
    "        dem_error = normalized_roughness * std\n",
    "        return dem_error\n",
    "        \n",
    "def f_linear(t, times_array, total_time_val):\n",
    "    if total_time_val == 0: return 0\n",
    "    return t / total_time_val\n",
    "\n",
    "def f_log(t, times_array, total_time_val):\n",
    "    if total_time_val <= 0: return 0\n",
    "    if t <= 0: t = 1e-6 \n",
    "    min_time_in_series = times_array[0] if len(times_array)>0 and times_array[0] > 0 else 1.0\n",
    "    adjusted_t = (t - min_time_in_series) + 1\n",
    "    adjusted_total_time = (total_time_val - min_time_in_series) + 1\n",
    "    if adjusted_total_time <= 1:\n",
    "        return 1.0 if t >= total_time_val else (t/total_time_val if total_time_val > 0 else 0)\n",
    "    val = np.log1p(max(0, adjusted_t-1)) / np.log1p(max(1e-7, adjusted_total_time-1))\n",
    "    return min(max(0, val), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3819c4f-1168-4c95-ac16-8d2c1b0b9388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manav\\AppData\\Local\\Temp\\ipykernel_10732\\394173793.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(baselines_path, delim_whitespace=True, header=None, usecols=[col_to_use])\n",
      "C:\\Users\\manav\\AppData\\Local\\Temp\\ipykernel_10732\\394173793.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(baselines_path, delim_whitespace=True, header=None, usecols=[col_to_use])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data generation. Files will be saved to: Datasets/TopographicData\n",
      "Generating data for config: lin_asc_16_s1\n",
      "Saved sample 10...\n",
      "Saved sample 20...\n",
      "Saved sample 30...\n",
      "Finished generating for config: lin_asc_16_s1. Total samples so far: 30\n",
      "Generating data for config: lin_desc_16_s1\n",
      "Saved sample 40...\n",
      "Saved sample 50...\n",
      "Saved sample 60...\n",
      "Finished generating for config: lin_desc_16_s1. Total samples so far: 60\n",
      "Generating data for config: lin_asc_16_p2\n",
      "Saved sample 70...\n",
      "Saved sample 80...\n",
      "Saved sample 90...\n",
      "Finished generating for config: lin_asc_16_p2. Total samples so far: 90\n",
      "Generating data for config: lin_desc_16_p2\n",
      "Saved sample 100...\n",
      "Saved sample 110...\n",
      "Saved sample 120...\n",
      "Finished generating for config: lin_desc_16_p2. Total samples so far: 120\n",
      "Generating data for config: log_asc_16_s1\n",
      "Saved sample 130...\n",
      "Saved sample 140...\n",
      "Saved sample 150...\n",
      "Finished generating for config: log_asc_16_s1. Total samples so far: 150\n",
      "Generating data for config: log_desc_16_s1\n",
      "Saved sample 160...\n",
      "Saved sample 170...\n",
      "Saved sample 180...\n",
      "Finished generating for config: log_desc_16_s1. Total samples so far: 180\n",
      "Generating data for config: log_asc_16_p2\n",
      "Saved sample 190...\n",
      "Saved sample 200...\n",
      "Saved sample 210...\n",
      "Finished generating for config: log_asc_16_p2. Total samples so far: 210\n",
      "Generating data for config: log_desc_16_p2\n",
      "Saved sample 220...\n",
      "Saved sample 230...\n",
      "Saved sample 240...\n",
      "Finished generating for config: log_desc_16_p2. Total samples so far: 240\n",
      "Data generation complete. Total samples: 240. Manifest saved to Datasets/TopographicData\\manifest.csv\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (1500, 1500)\n",
    "S_MAX = 5.0\n",
    "D_DEPTH = 50\n",
    "NU = 0.25\n",
    "CM = 1.0\n",
    "V_PARAM = 1.0\n",
    "RANDOM_NOISE_STD = 0.56\n",
    "TROPOSPHERIC_NOISE_BETA = 1.82\n",
    "TROPOSPHERIC_NOISE_SCALE = 1.0\n",
    "TOTAL_DAYS = 1460\n",
    "INTERVAL_DAYS = 49\n",
    "\n",
    "PRECOMPUTED_DATA_ROOT = r\"Datasets/TopographicData\"\n",
    "\n",
    "def LoadDem(dem_path):\n",
    "    data_type = np.dtype('>f4')\n",
    "    raw_data = np.fromfile(dem_path, dtype=data_type)\n",
    "    dem_data = raw_data.reshape((12602, 11702))\n",
    "    return dem_data\n",
    "\n",
    "def LoadBaselines(baseline_type='Sentinel-1'):\n",
    "    if baseline_type == 'Sentinel-1': \n",
    "        baselines_path = 'baselines_sentinel1.txt'\n",
    "        wavelength = 0.056\n",
    "        col_to_use = 2\n",
    "    elif baseline_type == 'PALSAR-2': \n",
    "        baselines_path = 'baselines_PALSAR-2.txt'\n",
    "        wavelength = 0.24\n",
    "        col_to_use = 1\n",
    "    df = pd.read_csv(baselines_path, delim_whitespace=True, header=None, usecols=[col_to_use])\n",
    "    baselines_list = df.iloc[:, 0].values\n",
    "    centered_baselines = baselines_list - np.mean(baselines_list)\n",
    "    return centered_baselines.tolist()\n",
    "\n",
    "DEM = LoadDem('Kyoto-Osaka.dehm') \n",
    "SENTINEL1_BASELINES = LoadBaselines(baseline_type='Sentinel-1')\n",
    "PALSAR2_BASELINES = LoadBaselines(baseline_type='PALSAR-2')\n",
    "\n",
    "def generate_and_save_data():\n",
    "    print(f\"Starting data generation. Files will be saved to: {PRECOMPUTED_DATA_ROOT}\")\n",
    "    os.makedirs(os.path.join(PRECOMPUTED_DATA_ROOT, 'noisy1'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(PRECOMPUTED_DATA_ROOT, 'noisy2'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(PRECOMPUTED_DATA_ROOT, 'clean'), exist_ok=True)\n",
    "\n",
    "    manifest_data = []\n",
    "    global_sample_idx = 0\n",
    "\n",
    "    dataset_configs = [\n",
    "        # Linear ('f_t': -1), Sentinel-1\n",
    "        {'name': 'lin_asc_16_s1', 'f_t': -1, 'orbit': 'ascending', 'interval': 16, 'baselines': SENTINEL1_BASELINES, 'wavelength': 0.056},\n",
    "        {'name': 'lin_desc_16_s1', 'f_t': -1, 'orbit': 'descending', 'interval': 16, 'baselines': SENTINEL1_BASELINES, 'wavelength': 0.056},\n",
    "\n",
    "        # Linear ('f_t': -1), PALSAR-2\n",
    "        {'name': 'lin_asc_16_p2', 'f_t': -1, 'orbit': 'ascending', 'interval': 16, 'baselines': PALSAR2_BASELINES, 'wavelength': 0.24},\n",
    "        {'name': 'lin_desc_16_p2', 'f_t': -1, 'orbit': 'descending', 'interval': 16, 'baselines': PALSAR2_BASELINES, 'wavelength': 0.24},\n",
    "\n",
    "        # Log ('f_t': f_log), Sentinel-1\n",
    "        {'name': 'log_asc_16_s1', 'f_t': f_log, 'orbit': 'ascending', 'interval': 16, 'baselines': SENTINEL1_BASELINES, 'wavelength': 0.056},\n",
    "        {'name': 'log_desc_16_s1', 'f_t': f_log, 'orbit': 'descending', 'interval': 16, 'baselines': SENTINEL1_BASELINES, 'wavelength': 0.056},\n",
    "\n",
    "        # Log ('f_t': f_log), PALSAR-2\n",
    "        {'name': 'log_asc_16_p2', 'f_t': f_log, 'orbit': 'ascending', 'interval': 16, 'baselines': PALSAR2_BASELINES, 'wavelength': 0.24},\n",
    "        {'name': 'log_desc_16_p2', 'f_t': f_log, 'orbit': 'descending', 'interval': 16, 'baselines': PALSAR2_BASELINES, 'wavelength': 0.24},\n",
    "    ]\n",
    "\n",
    "    for config in dataset_configs:\n",
    "        print(f\"Generating data for config: {config['name']}\")\n",
    "        gen_dataset = Noise2NoiseDataset(\n",
    "            size=IMG_SIZE, S_max=S_MAX, D=D_DEPTH, nu=NU, cm=CM, V=V_PARAM,\n",
    "            random_noise_std=RANDOM_NOISE_STD, tropospheric_noise_beta=TROPOSPHERIC_NOISE_BETA,\n",
    "            tropospheric_noise_scale=TROPOSPHERIC_NOISE_SCALE, total_days=TOTAL_DAYS,\n",
    "            interval_days=INTERVAL_DAYS, f_t=config['f_t'], orbit_type=config['orbit'], baselines=config['baselines'], \n",
    "            full_dem=DEM, wavelength=config['wavelength'])\n",
    "        \n",
    "        if len(gen_dataset) == 0:\n",
    "            print(f\"Warning: No samples generated for config {config['name']}. Check total_days and interval_days.\")\n",
    "            continue\n",
    "\n",
    "        for i in range(len(gen_dataset)):\n",
    "            try:\n",
    "                noisy1_tensor, noisy2_tensor, clean_tensor = gen_dataset[i]\n",
    "\n",
    "                noisy1_fname = os.path.join('noisy1', f'sample_{global_sample_idx:06d}_noisy1.pt')\n",
    "                noisy2_fname = os.path.join('noisy2', f'sample_{global_sample_idx:06d}_noisy2.pt')\n",
    "                clean_fname = os.path.join('clean', f'sample_{global_sample_idx:06d}_clean.pt')\n",
    "\n",
    "                torch.save(noisy1_tensor, os.path.join(PRECOMPUTED_DATA_ROOT, noisy1_fname))\n",
    "                torch.save(noisy2_tensor, os.path.join(PRECOMPUTED_DATA_ROOT, noisy2_fname))\n",
    "                torch.save(clean_tensor, os.path.join(PRECOMPUTED_DATA_ROOT, clean_fname))\n",
    "\n",
    "                manifest_data.append({\n",
    "                    'id': global_sample_idx,\n",
    "                    'config_name': config['name'],\n",
    "                    'original_idx_in_config': i,\n",
    "                    'time_step': gen_dataset.times[i],\n",
    "                    'noisy1_path': noisy1_fname,\n",
    "                    'noisy2_path': noisy2_fname,\n",
    "                    'clean_path': clean_fname\n",
    "                })\n",
    "                global_sample_idx += 1\n",
    "                if global_sample_idx % 10 == 0:\n",
    "                    print(f\"Saved sample {global_sample_idx}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating/saving sample {global_sample_idx} (original index {i} in config {config['name']}): {e}\")\n",
    "                continue\n",
    "        print(f\"Finished generating for config: {config['name']}. Total samples so far: {global_sample_idx}\")\n",
    "\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    manifest_path = os.path.join(PRECOMPUTED_DATA_ROOT, 'manifest.csv')\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    print(f\"Data generation complete. Total samples: {global_sample_idx}. Manifest saved to {manifest_path}\")\n",
    "\n",
    "generate_and_save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KyotoUniverstyResearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
